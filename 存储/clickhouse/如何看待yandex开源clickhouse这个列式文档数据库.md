# 如何看待yandex开源clickhouse这个列式文档数据库?

Yandex在2016年6月15日开源了一个数据分析的数据库，名字叫做ClickHouse，这对保守俄罗斯人来说是个特大事。更让人惊讶的是，**这个列式存储数据库的跑分要超过很多流行的商业MPP数据库软件，例如Vertica。**如果你没有听过Vertica，那你一定听过 Michael Stonebraker，2014年图灵奖的获得者，PostgreSQL和Ingres发明者（Sybase和SQL Server都是继承 Ingres而来的）, Paradigm4和SciDB的创办者。Michael Stonebraker于2005年创办Vertica公司，后来该公司被HP收购，HP Vertica成为MPP列式存储商业数据库的高性能代表，Facebook就购买了Vertica数据用于用户行为分析。简单的说，ClickHouse作为分析型数据库，**有三大特点：一是跑分快， 二是功能多 ，三是文艺范****1. 跑分快： ClickHouse跑分是Vertica的5倍快：**ClickHouse性能超过了市面上大部分的列式存储数据库，相比传统的数据ClickHouse要快100-1000X，ClickHouse还是有非常大的优势：**100Million 数据集:**ClickHouse比Vertica约快5倍，比Hive快279倍，比My SQL快801倍**1Billion 数据集:**ClickHouse比Vertica约快5倍，MySQL和Hive已经无法完成任务了
**2. 功能多：ClickHouse支持数据统计分析各种场景**- 支持类SQL查询，- 支持繁多库函数（例如IP转化，URL分析等，预估计算/HyperLoglog等）- 支持数组(Array)和嵌套数据结构(Nested Data Structure)- 支持数据库异地复制部署
**3.文艺范：\**目前\**ClickHouse的限制很多，生来就是为小资服务的**- 目前只支持Ubuntu系统- 不提供设计和架构文档，设计很神秘的样子，只有开源的C++源码- 不理睬Hadoop生态，走自己的路
**谁在用ClickHouse?**由于项目今年6月才开源，因此外部商业应用并不多件，但是开发社区的讨论还是保持热度(主要用俄语）
Yandex有十几个项目在用使用ClickHouse，它们包括：Yandex数据分析，电子邮件，广告数据分析，用户行为分析等等2012年，欧洲核子研究中心使用ClickHouse保存粒子对撞机产生的大量实验数据，每年的数据存储量都是PB级别，并支持统计分析查询
**ClickHouse最大应用：**最大的应用来自于Yandex的统计分析服务Yandex.Metrica，类似于谷歌Analytics(GA)，或友盟统计，小米统计，帮助网站或移动应用进行数据分析和精细化运营工具，据称Yandex.Metrica为世界上第二大的网站分析平台。ClickHouse在这个应用中，部署了近四百台机器，每天支持200亿的事件和历史总记录超过13万亿条记录，这些记录都存有原始数据（非聚合数据），随时可以使用SQL查询和分析，生成用户报告。

**ClickHouse就是快：比Veritca快约5倍**下面是100M数据集的跑分结果：ClickHouse 比Vertia快约5倍，比Hive快279倍，比My SQL 快801倍；虽然对不同的SQL查询，结果不完全一样，但是基本趋势是一致的。ClickHouse跑分有多块？ 举个例子：ClickHouse 1秒，Vertica 5.42秒，Hive 279秒；![img](https://pic2.zhimg.com/80/4a281b12b3bd534f812a8a058c37fca5_1440w.jpg?source=1940ef5c)**ClickHouse是什么，适合什么场景？**到底什么是ClickHouse数据库，场景应用是什么，参考下面说明：
![img](https://pic4.zhimg.com/80/67eecf7262927f728dd1c00aa9e0fe46_1440w.jpg?source=1940ef5c)**ClickHouse的不完美：**不支持Transaction：想快就别想Transaction聚合结果必须小于一台机器的内存大小：不是大问题缺少完整的Update/Delete操作支持有限操作系统开源社区刚刚启动，主要是俄语为主

**ClickHouse和一些技术的比较****1.商业OLAP数据库**例如：HP Vertica, Actian the Vector,区别：ClickHouse是开源而且免费的
**2.云解决方案**例如：亚马逊RedShift和谷歌的BigQuery区别：ClickHouse可以使用自己机器部署，无需为云付费
**3.Hadoop生态软件**例如：Cloudera Impala, Spark SQL, Facebook Presto , Apache Drill区别：-ClickHouse支持实时的高并发系统-ClckHouse不依赖于Hadoop生态软件和基础-ClickHouse支持分布式机房的部署
**4.开源OLAP数据库**例如：InfiniDB, MonetDB, LucidDB区别：这些项目的应用的规模较小，并没有应用在大型的互联网服务当中，相比之下，ClickHouse的成熟度和稳定性远远超过这些软件。
**5.开源分析，非关系型数据库**例如：Druid , Apache Kylin区别：ClickHouse可以支持从原始数据的直接查询，ClickHouse支持类SQL语言，提供了传统关系型数据的便利。
**第二章，死而后生**
ClickHouse设计之初就是为Yandex.Metrika而生，先一起看看Yandex.Metrika数据分析系统的演化过程吧，ClickHouse是第四代的解决方案，经过三次死亡后的产物，涅槃重生的巨兽！

**第一阶段：MyISAM (LSM-Tree) (2008-2011)**
Yandex.Metrika产品成立于2008年，最开始使用了MyISAM作为存储引擎。熟悉MySQL的同学都知道，这是MySQL的重要存储引擎之一（另外一个是InnoDB)。MyISAM中的实现也是使用LSM-Tree的设计，基本思路就是将对数据的更改hold在内存中，达到指定的threadhold后将该批更改批量写入到磁盘，在批量写入的过程中跟已经存在的数据做rolling merge。![img](https://pic4.zhimg.com/80/a955eb5091be05f0b890ac9e99c58b40_1440w.jpg?source=1940ef5c)
使用MyISAM的方法，刚开始数据量不大，访问请求也不大的时候，这个方法非常有效，特别是对于一些固定的报告生成，效率非常高，系统能够保持很好的系统写能力。
数据格式也是传统的索引结构：一个数据文件+一个索引结构； 索引结构是一个B-Tree结构，叶子节点保持着数据文件的OffSet; 通过Index文件找到数据范围，然后进行数据文件读取；早期的实现是将Index文件装在内存中，数据文件在磁盘当中，或则SSD等。当时7200RPM的硬盘，每秒进行100-200次随机读；SSD硬盘可以支持30000次随机读/每秒。
除了考察MyISAM之外，InnoDB也被考察过。MyISAM的索引和数据是分开的，并且索引是有压缩的，这种方式可以提高内存的使用率。加载更多索引到内存中，而Innodb是索引和数据是紧密捆绑的，没有使用压缩的情况下，InnoDb的大小会比MyISAM体积大很多。当然，InnoDB支持的Transaction也是非常诱人的。![img](https://pic1.zhimg.com/80/aef40277f47a7d7d04f70ef29934aa7a_1440w.jpg?source=1940ef5c)
**阶段二: Metrage (从2010-现在)**

为了解决MyISAM的一些问题，Yandex决定开发Metrage，核心想法来源于统计分析数据的一些特点，统计分析数据的每行数据量都不大，因此可以将多行数据聚合在一起作为处理单位，加快操作速度和系统的吞吐能力。
**它有几个特点：**数据通过小批量Batch存储支持高强度的写操作（数千行写入/每秒）读数据量非常小读数据操作中Primary Key 的数量有限（<1百万）每一行的数据量很小整个结构类似于MyISAM的索引，但是数据块中也聚合了一些小粒度的数据，索引放在内存中，数据被整理成块放在磁盘中，并且进行压缩。**该数据结构的优点：**- 数据被压缩成块。 由于存储有序，压缩足够强大，其中使用了快速压缩算法（在2010年使用QuickLZ ，自2011年使用LZ4 ）。- 采用稀疏索引： 稀疏索引 - 主键值排序后放置于若干个组中，可以节省大量索引空间。 这个索引始终放在内存中。
Metrage在数据量最大的时候，39*2台服务器中存储了大约3万亿行数据，每天机器处理大约为1千亿的数据。
这个系统有个缺点，数据查询只能进行基于固定的查询模式（否则性能将受到很大影响），因此在设计数据Schema的时候，需要考虑数据查询的性能问题，缺少足够的灵活型。因此这个项目使用了5年后，统计分析的数据都开始迁移到其他的平台系统中了（那时候LevelDB，还没有出现，否则可以使用LevelDB作为Mertage的核心模块）。
**阶段三 OLAPServer (2009-2013)**
随着Yandex.Metrike的数据量越来越大，数据查询的速度越来越慢，查询相应事件长，系统的CPU和IO资源占用大，因此公司内部尝试了不同的解决方案，其中一个原型方案是OLAPServer。 设计思路就是根据“星型结构”设计一些维度和事实列，通过预先部分聚合数据加快访问的速度，这一套技术用于支持各种报告的生成。

**基本的场景如下：**
支持一个Fact表，包括维度列(Dimension)和指标列（Metrics)，维度有上百个读取大量行的数据，但是一次查询往往只关注某些列写多读少的场景，报表查询请求量并不大大部分简单查询不超过50毫秒响应时间列的值数据量非常小，通常为整数或者不超过60字节的URL它需要高带宽，同时处理单个请求（高达十亿每秒的行的单个服务器上）查询结果的数据量非常小，通常是数据聚合的结果无需支持事务，数据更新极少，通知只有添加操作
这些场景下，使用列式数据库是非常有效的，从两个方面可以理解
**1. 磁盘I/O的优化**- 作为列式存储，查询只需要访问所关心的列数据- 列数据放在一起，数据格式类似，非常容易压缩，因此减少I/O数据量- 输入输出的减少，内存可以腾出更多地方作为Cache**2. CPU**由于数量行数特别大，数据的解压缩和计算将耗费非常多的CPU资源，为了提高CPU的效率，行业中通常是将数据转换成Vector的计算。例如行业比较流行的VectorWise方法。
下面是VectorWise的高层架构示意图，其基本想法就是将压缩的列数据整理成现代CPU容易处理的Vector模式，利用现代CPU的多线程，SIMD(Single Instruction,Multiple Data)，每次处理都是一批Vector数据，极大的提高了处理效率。![img](https://pic2.zhimg.com/80/7079b8a81804ee14b7e3eb3f1e5aa354_1440w.jpg?source=1940ef5c)市场有非常多的的列式分析型数据库，例如HP Vertica, ParAccel Actian the Matrix, Google PowerDrill , Amazon的RedShift , MetaMarkets Druid等等，这些产品有很多不同的优化实践，有些是专于数据压缩，有些是专于数据聚合，有些是专于扩展性等。OLAPServer在具体实现过程中，实际上采用的是比较保守的方法，实现的功能也比较有限，但是完全满足当时分析报表的支持。例如，OLAPServer数据类型只支持1-8字节的数据类型，查询只支持固定的模式：
**Select** keys ,aggregate(columns) **from** table **where** condition1 and condition2 .... **Group by**keys **order by** columns 。

尽管功能有限，OLAPServer还是满足了当时的分析报表功能，并且性能非常出色。由于设计之处的限制比较多，因此后期的改进过程中成本非常高，例如为了增加更长URL的数据类型，系统改动非常大。在2013年，OLAPServer存储了7280亿行数据，目前这些数据都迁移到ClickHouse了。
**第四阶段 ClickHouse（2011-现在）**使用OLAPServer，我们能够可以实时看到一些预先聚合的数据，但是对于一些聚合前的详细数据是无法查询的，随着业务的深入发展，精细化运营对于统计服务提出了更高的要求，后期有大量需求是关于直接查询聚合前的数据。
总体来说，虽然数据聚合带来一些好处，但是也存在以下一些问题。
对于基数大的列，聚合的意义不大，例如URL等
过多的维度组合会导致组合爆炸用户常常只关心聚合后的数据中的非常一一小部分数据，因此大量聚合预计算是得不偿失的。聚合后的数据，数据修改会非常困难，很难保证存储的逻辑完整性
如果不预先聚合数据，如何保证响应时间是一个大挑战。这意味着，数据库需要支持秒级处理数十亿的行。
近年来，市面上也出现很多列式存储的开源DBMS，包括Cloudera Impala, Spark SQL, Presto, Apache Drill，这些系统虽然都能完成查询的功能，但是速度却无法满足数据统计分析的需求，即使聚合后的性能能够满足，但也缺少灵活度。
**因此，Yandex开发了自己的列式分析数据库 ClickHouse，初期主要是满足Yandex.Metrike的统计分析需求，主角要上场了。**ClickHouse实际上来源于内部的几个项目的整合，项目起源起源于2011年左，
到2013年的时候，ClickHouse的性能就和Vertica大致相同；2015年12月，ClickHouse的数量已经达到11万亿行，数据表有200多列，主集群的服务器数量也从初期的60台到394台；整个系统的部署是支持水平扩展的，并且支持多机房部署和备份。虽然它是能够在大型集群操作，它可以被安装在同一服务器上，甚至在虚拟机上。
在最新的性能评测中，ClickHouse比Vertica快约5倍。现在Yandex公司内部有十几个应用系统在使用ClickHouse，场景包括数据存储，查询分析，报表制作等。
**ClickHouse的蓝图**
关于ClickHouse的下一步发展，公司并没有给出太多规划，因为多数信息还是属于不公开状态，但是从一些公开的信息，我们可以了解到，ClickHouse会向两个方向发展。**1 云计算数据库：**　
Yandex希望通过ClickHouse促进公司云计算数据库的发展，包括用户可以通过云服务的方式，使用ClickHouse，开源是走向市场的第一步。
**2. 加强SQL兼容性。**
为了支持更多的企业用户，目前的查询虽然采用非常近似的SQL语言，但是还有很多地方需要改进，包括和一些商业软件（例如Tableau,Pentaho）的集成无缝使用。**第三部分：遥指杏花村**
这一部分包括了一些ClickHouse的一些基本信息，帮助大家进入ClickHouse的世界。为了深度了解ClickHouse社区，不仅仅需要翻墙，也需要谷歌或者必应的翻译器，俄文翻译的效果不错。
１主页：　[https://clickhouse.yandex](https://link.zhihu.com/?target=https%3A//clickhouse.yandex)
２.代码：　[GitHub - yandex/ClickHouse: ClickHouse is a free analytic DBMS for big data.](https://link.zhihu.com/?target=https%3A//github.com/yandex/ClickHouse)３参考文章：**Yandex.Metrike的架构演化：**
[Эволюция структур данных в Яндекс.Метрике / Блог компании Яндекс / Хабрахабр](https://link.zhihu.com/?target=https%3A//habrahabr.ru/company/yandex/blog/273305/)（俄文）很棒的文章**MPP数据库基础架构：**
[http://vldb.org/pvldb/vol5/p1790_andrewlamb_vldb2012.pdf](https://link.zhihu.com/?target=http%3A//vldb.org/pvldb/vol5/p1790_andrewlamb_vldb2012.pdf)
[http://www.cs.yale.edu/homes/dna/talks/Column_Store_Tutorial_VLDB09.pdf](https://link.zhihu.com/?target=http%3A//www.cs.yale.edu/homes/dna/talks/Column_Store_Tutorial_VLDB09.pdf)**４关于Yandex的**
Yandex的（纳斯达克股票代码：YNDX）是互联网公司在俄罗斯主导，经营该国最流行的搜索引擎和访问量最大的网站。Yandex的还经营在乌克兰，哈萨克斯坦，白俄罗斯和土耳其。Yandex的的使命是回答任何互联网用户的任何问题（*Answer any question Internet users may have*)。最近在学习一些ClickHouse的源代码，还没有理清楚头绪，下次搞清楚逻辑后再和大家介绍一下，这里先纸上谈兵，点到为止了。
---------------------完------------------------------

[发布于 2016-08-24](https://www.zhihu.com/question/47604963/answer/118737995)

赞同 24312 条评论

分享

收藏喜欢收起

继续浏览内容

![img](https://pic4.zhimg.com/80/v2-88158afcff1e7f4b8b00a1ba81171b61_720w.png)

知乎

发现更大的世界

打开

![img](https://pic2.zhimg.com/80/v2-da7d9e4b6a7ddba507299bcf5a4d0600_1440w.png)

Safari

继续

[![腾讯技术工程](https://pic1.zhimg.com/v2-0a172693e441712c5f687d23fc187717_xs.jpg?source=1940ef5c)](https://www.zhihu.com/org/teng-xun-ji-zhu-gong-cheng)

[腾讯技术工程](https://www.zhihu.com/org/teng-xun-ji-zhu-gong-cheng)[](https://www.zhihu.com/question/48510028)

已认证的官方帐号

64 人赞同了该回答

分享一下 Clickhouse 在腾讯的两点应用实践： Clickhouse 的部署和管理Clickhouse 在腾讯业务线的应用实践**Clickhouse 的部署和管理**Clickhouse 它自身是一个非常强大的数据处理的引擎，因为它非常专注数据处理的计算效率这一块，因此它周边的一些管理插件，其实还是比较弱的。 大家在做大数据的平台，以及在做一些平台产品的时候，其实管理和监控都是蛮重要的一部分。不然你的平台出了问题，业务提出挑战，说为什么查不出来，或者我的数据为什么慢？这时候平台如果没有一个合理的解释的话，平台必然是要背锅的。 下面我们一起看一下 Clickhouse 在我们生产环境的部署情和监控管理。先给大家介绍一下我们的选型机器，因为 Clickhouse 它是一个服务于大数据场景的MPP数据库，那么它的应用场景是非常明确的，是一个“大数据”的快速查询场景。 这种情况下，其实它对于并发性要求不是特别的高，因为数据大，那么它对于磁盘的容量其实要求还反倒多一些，这样的话我们用一些廉价盘就是传统的这种sata盘来做这种的底层数据的存储。 Clickhouse 虽然有这种主从同步，数据复制的机制，它的一个单机损坏的话，虽然你有备库可以做支撑查询，但是相对来说恢复的成本也是很高的。建议在单机之上使用raid5模式，也就是尽量在单机况下也能够保障它一些数据的安全性。 然后因为 Clickhouse 还有一个特性，就是它是一个share nothing的架构，就是说他不能避免一些情况下需要单机一些内存的排序的和处理一些大量的数据，这种情况下，它对于内存的要求其实也蛮高的，有的时候像内存如果不够它会出现oom，虽然他有一些可以扩展内存的方式使用磁盘去做一些额计算出来，但是相对来说性能会有影响。所以会选择内存稍微大一点的服务器。然回到网卡话，因为万兆网卡都是比较普及的了，千兆网卡在大数据应用的场景下是非常容易被打满的，容易影响到服务的稳定性，因此选择万兆网卡作为基础服务。 下面这是我们一个生产环境的部署方案：![img](https://pic4.zhimg.com/80/v2-44dad1932d030704a602500d5417eb3f_1440w.jpg?source=1940ef5c)首先笼统的说我们是一个读写分离的模式，数据写入我们使用这样一个外围的负载均衡机制来去写不同的分片，然后不同分配下都是双重备份的，这样的话就可以分散的去写数据。 读的时候是用distribution表去读取数据，对 Clickhouse 有一定了解的话，就会疑惑，我们为什么不直接去写这个distribution表，这样不是峰更方便吗？这有有两点原因。**第一点就是专一**distribution表更多的工作应该是分发读取SQL和整合数据，是在读取的方面比较有优势，而非要利用它来做负载均衡进行数据写入，虽然可以，但是无意中增加的它的负担，再说外部独立部署一个负载均衡是一个非常简单的事情。把专有功能交给专有模块去做会更加合适，也更加稳定一些。 **第二点就是在扩容方面比较平滑**因为 Clickhouse 比较粗犷，如果直接写distribution表的话，在扩容过程中难免会遇到一些问题，不够平滑。而这种方案可以按照以下步骤更加平滑的扩展服务：安装新部署新的shard分片机器批量修改当前集群的配置文件增加新的分片新shard上创建表结构名字服务添加节点![img](https://pic2.zhimg.com/80/v2-35128bddee27e4929f6b01d0682a4c22_1440w.jpg?source=1940ef5c)在这个过程中你在做前三步的时候，对在线服务基本上是没有任何影响的OK了，并且你新上的节点也是没有任何数据写入的，当你完成了所有的检查之后，在负载均衡器中增加你的新节点，就能够保障一个平滑扩容了。 下面是 Clickhouse 写入策略的一个小分析：![img](https://pic1.zhimg.com/80/v2-cd07c380ed9c2f641ccafcf32bc86758_1440w.jpg?source=1940ef5c)Clickhouse 不是事务型的那种数据库它无法支持很高的并发，它服务于大数据，它更加适合这“大批量，少批次”的写入，所谓的大批量少批次，就是说把数据一批一批地组合在一起，一次性的去写入更多的数据，在我们线上测试的数据中大家可以看到， 2万条一个批次和100万一个批次的差别，我们主要看两块，一是磁盘的等待，它们基本上都是50%左右。 一个服务器，磁盘50%等待的时候就已经说明他的IO已经到达瓶颈了，还算是一个可以忍受的瓶颈，但它们的差别在哪？当插入2万的时候40%的磁盘等待已经出现了insert失败的错误， 100万的时候磁盘等待时间是54%磁盘IO要比2万的时候高，但是没有任何插入失败，说明服务器的利用率更高了。因此可以验证， Clickhouse 更适合这种大批量，少批次的写入模式。 下面的例子比较类似，是在说每一条数据的大小情况，用10K 和 100 B对比，大家可以看到10K的数据相对来说更加适合 Clickhouse 的写入情况。这个场景能就反推过来，一个应用场景所有的数据库在处理join的时候其实都是有一些瓶颈的，但是 Clickhouse 的优势是列宽，他是一个列式存储，对列宽基本上没有什么限制，因此我们可以建立一个更宽的大宽表，吧join的操作变成表查询操作，我昨天和ivan聊了一下，目前 Clickhouse 的列宽大概可以支持到1万列左右，在1万个月以下的时候，其实没有什么问题，都是非常好的，因此建议大家更多使用宽表来替代join。以上介绍了 Clickhouse 的部署模式和写入模式，下面介绍一个立体监控模型。**Clickhouse 立体监控模型**![img](https://pic1.zhimg.com/80/v2-b72fab96804303653ab1e9c2957d25f9_1440w.jpg?source=1940ef5c)大家在做平台的时候，很多情况下就会有一个分界点，就是跟业务之间分得太开，业务做业务的数据指标监控，平台做平台的监控。但其实平台有时也需要关注业务的这种指标数据，你业务数据出现了问题，反推平台一定是也出现了问题的。所以这种情况下就是说做平台的兄弟一定也是要建立一些业务方面的监控。例如空值或者是断线这样的一些监控指标其实是非常重要的。然后在服务层，就是 Clickhouse 。很多时候不管是 Clickhouse 也好还是一些其他服务，大家很容易忽略它的错误日志，偶尔几条错误服务不crash的情况下大家关注不多。这种情况其实无意中就给你的服务埋下了一个很大的隐患，所谓千里之提溃于蚁穴。 ![img](https://pic2.zhimg.com/80/v2-f1f12079f7f428cac7bc0fbd02fb3929_1440w.jpg?source=1940ef5c)![img](https://pic4.zhimg.com/80/v2-f926516f81cb0500d726c7bcf2bf5928_1440w.jpg?source=1940ef5c)这种情况下，建议大家把log不断的记下来，当你的服务出现问题的时候完全可以去看出问题的时间点，你的错误log到底有多少？你的log内容到底是什么？这样可以提高你平台处问题时候的定位效率，也能提高你的平台的稳定性。再往下的话是这样的，是一个业务请求指标，这样一个请求指标的监控平台上有的时候也会容易忽略的。比如业务忽然找到平台，说我的查询慢了，我的插入太慢了，为什么？这个时候就有了业务请求指标的话，你可以跟他今天，昨天，上一个小时数据数据去做对比，比如插入的数据是昨天十倍或者是三五倍，那平台如果慢了，就可以明确的说明原因，是业务增长导致的，需要进行优化或者进行扩容，这样对于业务它有一个平台使用的预期，对平台的信任感也会增加，而不会因为只是发现过载了，就去扩容，盲目的去找了一圈原因也不知道为什么过载了。 下面是扫描详情的这种监控，刚才说的前面的那几种监控，例如错误日志暴涨，负载高了，这种情况基本说明平台已经出问题了，更多的时候需要防患于未然。这就需要下面的扫描结果集监控和查询耗时的监控，属于亚健康性状态的监控，就说作为一个 Clickhouse 服务平台，要经常去关注一下，查询的扫描集和结果集，以及它的查询耗时，提前把查询耗时长，结果集大的询亚健康任务找出来进行优化，省得慢查询造成服务的堵塞把平台拖死影响到业务，特别是混合业务的支撑很容易因为， A业务把平台打垮了，B业务找过来说我的业务可用了，比起出现问题后大家手忙脚乱的解决问题，这种亚健康状态监控是非常重要。 ![img](https://pic4.zhimg.com/80/v2-ef0eaf1ba860538052b5d175c50f581d_1440w.jpg?source=1940ef5c)![img](https://pic4.zhimg.com/80/v2-3b378f7465bcee25c061db34885759a3_1440w.jpg?source=1940ef5c)经过上面举例，归纳起来就是这样一个立体监控模型，我们从应用层，服务层，物理层。这样三层去把 Clickhouse 的监控做得更细一点。物理层这一块我刚才没有去举任何的例子了，因为这块东西是大家都比较通用的，就是磁盘的IO，持续的负载，还有流量。刚才有朋友问， Clickhouse 在cpu上面的消耗。其实 Clickhouse 对CPU的性能压榨是很高的，它的CPU偶尔飙到百分之百是在集中计算数据，他不像是事务性并发数据库那样需要支持高并发，因此对于波浪形的CPU彪高的监控并没有太大意义。 更重要的是IO的瓶颈。这种立体监控模型，对敏感度来说的话，肯定是应用层次最低的，就是说应用层出问题了，一定是就是说从底到上物理层，就是说首先是服务器的IO或者负载高了，然后影响到 Clickhouse 的写入慢，或者是查询耗时长了，或者写入失败了，然后再体现在业务指标出问题了，监控敏感度是一层层降低的，但所谓的监控的紧急度的话，反而是反过来是一个从高到低的，我业务处就是应用业务出问题了，一定是要第一时间去响应去解决这个问题， 已经影响到业务了，你的SLA 也就受到了挑战。相对于物理层和服务层的这种层级的监控，出现一些慢SQL，或者一些高负载的情况，我们是可以有时间提前去把这种亚健康的状态解决掉，更好的保障平台的稳定性。 ![img](https://pic2.zhimg.com/80/v2-0c0276fefce2b8e77fb18e45840afd10_1440w.jpg?source=1940ef5c)Clockhouse在性能上面是非常优秀的，但是技术圈里面没有银弹，无论什么东西，都需要合理的使用方式才能发挥出它最大的价值，希望大家能够在我的分享中得到一些启发让自己的 Clickhouse 平台更加稳定， 但是他同时就说是也需要大家有一个合理的使用方式和一个合理的管控方式，让这个平台更加稳定。**Clickhouse 在腾讯游戏业务线的应用实践**主要从四个方向进行介绍。首先是我们游戏数据分析的业务背景，我们为什么要在我们腾讯游戏中做一些数据分析，然后是我们自研的数据分析引擎TGMars，对于其中的画像系统使用的的是 Clickhouse 。我们也有自己的分布式查询画像服务，至于为什么要进行替换，后面也将从第三点进行一些介绍 ，最后简单的介绍下我们平台对 Clickhouse 的使用。给大家介绍一下，我们腾讯游戏数据分析的一个场景作图。然后从这张图上我们可以看出我们在底层的大数据基础平台上构建了一个pass应用平台，基于我们的pass能力构成了最上层的sass系统，游戏运营小伙伴可以在我们的系统上去实时的进行数据分析，任务配置，营销活动，从而提高我们的游戏服务质量。![img](https://pic2.zhimg.com/80/v2-a0dc7e4e614e90f21410c0404d73773b_1440w.jpg?source=1940ef5c)我们首先看下最下层的大数据基础平台，其主要分为数据的采集和存储。如果大家都玩游戏的化可能会有一个明显的感觉，如果游戏有几毫秒的延迟，体验起来就会特别差，所以数据的采集也是我们投入较大的一块。关于存储，我们会将数据落地到数据仓库tdw，同时也有我们也有自己的实时传输管道，保证数据实时的消费落地展示。在我们基础的数据处理平台上我们形成了一个pass化的大数据服务平台，主要分为三个方向。首先是我们的挖掘分推荐方向，通过对数据的处理训练达到精准的推荐，比较常见的是我们微信游戏中的游戏推荐。接着是我目前所在的idata产品中心，我们主要做游戏的实时数据的在线统计分析，比如我们的实时在线用户，实时游戏收入，实时的创角等，同时我们也提供用户的多维提取和画像分析等，在画像分析中，我们目前就是使用的 Clickhouse 。最后就是我DataMore的大数据应用服务，提供大数据的实时决策能力。下面对我们的数据分析引擎做些简单的介绍，比对于业界传统的大数据分析引擎，我们可以明显的发现，我们提供更多更强大的能力。传统的大数据分析引擎通过spark或者hadoop对数据进行多维的聚合操作，形成了自己的报表分析结果。我们根据游戏数据的特点构建了自己的快速分析引擎，首先是对spark的定制修改，形成了TGSpark，它在游戏的多维聚合下推过程中具有更好的运行效率，然后我们对数据也进行了特点的处理，构建了自己的存储系统。在我们的底层引擎上我们构建了自研的报表服务，可以像tableau一样进行数据的托拉拽和数据展示。当然多维分析，多维提取和用户画像的功能也是比不可少的。![img](https://pic2.zhimg.com/80/v2-826f45f20a7921fddff7f6081615ce39_1440w.jpg?source=1940ef5c)这里演示下我们的多维提取和透视分析，右上角的是我们的多维提取功能，用户通过指标选择他们想要提取的数据信息 ，比如最近流失的用户和 用户等级大于10级的用户，指标之间也可以进行各种组合，然后在我们的引擎上对结果进行快速的提取，提取到相关信息后用户可以对它进行画像分析，透视分析等操作。![img](https://pic1.zhimg.com/80/v2-4bd79089e83b9247ac40339a1071c174_1440w.jpg?source=1940ef5c)右下图是用户的透视分析，通过对指标的托拉拽，获取用户需要的结果。新的画像系统目前还在灰度阶段，目前只在游戏平台部进行小规模的使用。其实我们旧的画像系统多维下钻分析效率也蛮快的，对于亿级数据的10维以下的分析基本上是秒级分析出结果。 下面对我们的画像系统简单的介绍下，分析我们为什么要对其进行改造。我们画像系统的主要功能就是数据导入和数据的展示，因此整个画像系统的设计也是围绕这两个主要的功能进行的设计。系统主要分为三个方面，调度层，存储节点，执行节点。![img](https://pic4.zhimg.com/80/v2-a5a320510a8502fad5b486b17cc14b5c_1440w.jpg?source=1940ef5c)首先进行数据的导入时，我们会在调度服务上选择出主节点，对数据进行切分然后均匀的存储在各存储节点上，同时也会对数据进行一些元信息的统计，方便数据展示时快速获取一些关键信息，存储节点中我们将数据进行列式存储，以及数据的分片处理和压缩等。在数据展示层，用户可以通过托拉拽形成sql，我们主调度层会对sql进行解析，解析完成后我们对sql进行优化，形成我们的DAG执行计划下发到各直接节点进行查询，我们查询时会通过jit将sql转换层字节码信息，加快查询效率。同时在执行层中我们有自己的位图缓存技术，在每次下钻查询过程中会进行动态位图索引匹配，加快查询效率，最终由主节点进行查询结果汇总输出。![img](https://pic2.zhimg.com/80/v2-f11d591eeaba087f0488d7ba640a88f5_1440w.jpg?source=1940ef5c)我们的分布式查询服务引擎其实也是一个MPP架构模式，查询效率也很快为什么我们还要打算用 Clickhouse 进行替换？主要可能是因为下面几个原因，首先就是它的扩展性比较差，不支持数据的扩展和修改，如果用户在我们系统上提取一个用户包然后进行画像分析，假如是从去年9月10号到今年10月10号提到一些数据包，然后进行分析，过了几天他又想多分析几天的数据，比如从去年9月10号到10月20号，中间就是差10月10号到10月20号的数据，我们不支持的数据增加和修改，所以它只有重新提取，然后重新导入，所以说它的扩展性就是有些问题。然后数据类型只是数字类型，因为我们为了加快它的查询效率，那时候只做数字类型，因为我们画像时只需要做一些统计类的信息，但是随着业务方需求的变化，单一的数字类型已经不能满足我们的需求了。同时当数据量达到了10亿级别以上，维度分析达到十几维，效率就会有所下降，查询效率就基本上到了分钟级别。也是因为这些原因我们需要进行一些改造或者替换，后面发现对它改造的代价其实还是蛮大的，我们就去调研了一些主流的olap分析系统，最终我们被 Clickhouse 强大的功能所吸引，也就决定通过它进行替换。首先最重要一点应该可能就是它超高的查询效率，这里给了一个简单的多维下钻分析，在四亿的数据多维下钻分析中，在低纬度的分析中其实画像和 Clickhouse 都能在秒级内查询出结果，因为这里的对比图是单机运行的，所有比实际生产环境还是慢很多的，但是当维度增加时 Clickhouse 的强大之处就展示出来了。同时 Clickhouse 还有其它一些非常优秀的能力，比如它的SQL能力，聚合函数能力，还有它的目前正在完善的机器学习能力等等。建议大家也可以搭建个环境去体验下，使用起来真的很好，必须吹一下。![img](https://pic2.zhimg.com/80/v2-48d88c01d5c2687c4b633563a1d5cdfe_1440w.jpg?source=1940ef5c)然后下面介绍一下，通过一个实例介绍下我们是如何通过 Clickhouse 满足我们以前做起来比较麻烦的需求。比如说map类型的数据处理，我们接到业务方的数据格式是map类型，需要对其中的业务信息进行统计，这个map的key可以理解为游戏代号，value值可以理解为游戏的登录吧，在最开始的游戏登录统计分析中，我们时在tdw上进行计算，通过hive自带的sql对map进行切分然后进行汇总统计，数据量其实也就十几亿吧，但是整个结果计算下来竟然花费了十多分钟吧。![img](https://pic3.zhimg.com/80/v2-90e14fca41d8660224a32696b32c46f0_1440w.jpg?source=1940ef5c)然后我们将数据进行处理导入到我们画像系统中，也就是将map转换成可扩展的列，每个key值对应单独的一列，对每一列进行统计计算，这样查询效率是变快了，几秒中就能查询出结果，但是游戏数据增多我们需求在对映射关系进行一些修改，所有使用起来也就没那么方便。但是我们使用 Clickhouse 后发现处理这样的需求真是不要太简单了，它支持嵌套的数据类型和数组类型，因此我们选择数据导入时将其转换成对应的数据格式进行导入，然后进行一个简单的sql就能查询出最终的结果。下面这两个sql就是嵌套数据类型和数组类型的查询方式，sql统计非常简单，进行array join就能查询出每个key值sum的结果。通过查询结果我们也可以发现，它的效率也是非常高的，整个查询结果仅仅花费了0.3秒，这还是在我们集群比较小的情况下的结果。同时 Clickhouse 的其它一些函数也能满足我们的一些特殊需求，所有感觉非常棒。![img](https://pic4.zhimg.com/80/v2-9ef1b324ec58dff2811b1021d5f3042f_1440w.jpg?source=1940ef5c)后面说下我们目前对 Clickhouse 的使用。最下面的使我们不同类型的数据源，TDW数据源主要是hdfs出库文件，RDBMS数据源，我们自己的TGMars数据源，还有消息中间件数据源。目前消息中间件这快运用的比较少，因为我们做实时数据有自己的一套druid和storm，运行比较稳定，因此这块我们目前只做了些功能验证测试。接下来就是我们使用较多的tdw和tgmars数据源，我们通过自己的etl服务工具进行数据源的转换，因为 Clickhouse 其实适合大批量小批次的写所有我们通过etl工具进行控制，同时也会对数据进行监控，保证数据完整性。其实 Clickhouse 也直接MySql引擎，可以通过它实现一些小的功能，比如数据库处于不同实例中，我们可以通过 Clickhouse 进行跨实例的join 操作，这块我在测试使用时还是比较感兴趣的。数据导入到 Clickhouse 后，我们就将基于它做一些服务应用开发，比如我们的画像服务，BI系统，以及查询服务。当然我们也将打算通过它做一些增强分析相关的工作，目前第一步正在进行游戏指标的统计，接着可能会进行NLP处理，最终使用 Clickhouse 出结果。![img](https://pic4.zhimg.com/80/v2-494abfbb2fcb212e052899b67f28d97c_1440w.jpg?source=1940ef5c)最后就是我们近期的展望，后面会做一些增强分析的工资，就可能会用到更多的机器学习算法，刚刚从俄罗斯团队上的分享中也了解到目前他们也在这方面投入的更多力量，期望后续我们也能很好的使用上。然后就是执行计划的分析，其实这个也不算必须的，因为我们可以从query log中查看到对应的信息，但是用习惯explain后还是比较喜欢这种方式。最后就是集群管理方面，其实集群的搭建和管理还不是那么的方便，希望后续官网提供更好的方案供使用者使用。